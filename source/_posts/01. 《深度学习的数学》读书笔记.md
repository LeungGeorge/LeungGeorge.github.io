---
tags:
  - note
  - blog
  - 深度学习的数学
  - LLM
  - NLP
  - TODO
share: "true"
categories:
  - 读书笔记
title: 深度学习的数学
date: 2024-11-11,17:29
comments: true
number headings: auto, first-level 1, max 6, 1.1
---

# 1 第一章：神经网络的思想

## 1.1 深度学习里的“深度”是什么意思呢？

## 1.2 神经网络的基本原理？

从数学上来说，其原理十分容易。本书的目的就是阐明它的原理。可能后面会解答清楚。

## 1.3 单位阶跃函数

[单位阶跃函数](https://zh.wikipedia.org/zh-hans/%E5%8D%95%E4%BD%8D%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0)，又称赫维赛德阶跃函数，通常用 H 或 θ 表记，有时也会用 u、 1 或 𝟙 表记，**是一个由奥利弗·亥维赛提出的阶跃函数，参数为负时值为 0，参数为正时值为 1**。分段函数形式的定义如下： 另一种定义为： 它是个不连续函数，其微分是狄拉克 δ 函数。

> ([深度学习的数学.pdf, p.10](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=23&selection=275,0,275,6&color=yellow))
> 单位阶跃函数

![](assets/images/7daf74d7beeb14a0f22a8285a010602e_MD5.png)

单位阶跃函数的图形如下所示：

![](assets/images/07bf99eea13563a206b85fbe0be471bc_MD5.png)

## 1.4 Sigmoid 函数

> [!PDF|yellow] [深度学习的数学.pdf, p.11](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=24&selection=310,11,310,21&color=yellow)
>
> > Sigmoid 函数

S 型生长曲线。

[Sigmoid 函数](https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407) 也叫 [Logistic 函数](https://baike.baidu.com/item/Logistic%E5%87%BD%E6%95%B0/3520384?fromModule=lemma_inlink)，用于隐层神经元输出，取值范围为 (0,1)，它可以将一个实数映射到 (0,1) 的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid 函数为神经网络中的激励函数，是一种光滑且严格单调的饱和函数，其表达式为：

![](assets/images/5f3a4903c78383b881b3a0c10c47385f_MD5.png)

![](assets/images/51616d0e23a62d8964ec0d4aae01278e_MD5.png)

## 1.5 激活函数

在计算机网络中，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。

sigmoid 函数和 tanh 函数是研究早期被广泛使用的 2 种[激活函数](https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink)。两者都为 S 型饱和函数。当 sigmoid 函数输入的值趋于正无穷或负无穷时，梯度会趋近零，从而发生梯度弥散现象。Sigmoid 函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。Tanh 激活函数是 sigmoid 函数的改进版，是以零为中心的对称函数，收敛速度快，不容易出现 loss 值晃动，但是无法解决梯度弥散的问题。2 个函数的计算量都是指数级的，计算相对复杂。Softsign 函数是 tanh 函数的改进版，为 S 型饱和函数，以零为中心，值域为（−1，1）。

> [!PDF|yellow] [深度学习的数学.pdf, p.12](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=25&selection=6,0,7,10&color=yellow)
>
> > 激活函数：将神经元的工作一般化

![](assets/images/8a05ce9af6ed3aee6e2834912fbff3c5_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.14](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=27&selection=140,0,150,0&color=yellow)
>
> > 激活函数的代表性例子是 Sigmoid 函数 σ(z)

## 1.6 逻辑回归

[logistic 回归](https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92/2981575?fromModule=lemma_inlink)是一种广义线性回归（generalized linear model），因此与[多重线性回归](https://baike.baidu.com/item/%E5%A4%9A%E9%87%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/4029155?fromModule=lemma_inlink)分析有很多相同之处。

## 1.7 自变量、因变量

广义解释任何一个系统（或模型）都是由各种变量构成的，**当分析这些系统（或模型）时，可以选择研究其中一些变量对另一些变量的影响，那么选择的这些变量就称为自变量，而被影响的量就被称为因变量**。例如：分析人体这个系统中，呼吸对于维持生命的影响，那么呼吸就是自变量，而生命维持的状态被认为是因变量。

## 1.8 偏置

我们将 - θ 替换为 b。

> [!PDF|yellow] [深度学习的数学.pdf, p.16](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=29&selection=62,0,72,0&color=yellow)
>
> > 经过这样处理，式子变漂亮了，也不容易发生计算错误。这个 b 称为偏置（bias）

![](assets/images/9026eb1d57b432a91870d905579d84f7_MD5.png)

> 卧槽，为了变漂亮搞这么一手？

![](assets/images/f2b0014038c32f680d824585470cdaca_MD5.png)

## 1.9 内积

在[数学](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6 "数学") 中，**点积**（德语：Punktprodukt；英语：dot product）又称**数量积**或**标量积**（德语：Skalarprodukt；英语：scalar product），是一种接受两串等长的数字序列（通常是[坐标](https://zh.wikipedia.org/wiki/%E5%9D%90%E6%A0%87 "坐标")[向量](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F "向量")）、返回单一数字的[代数](https://zh.wikipedia.org/wiki/%E4%BB%A3%E6%95%B0 "代数")[运算](https://zh.wikipedia.org/wiki/%E8%BF%90%E7%AE%97 "运算")。

在[欧几里得几何](https://zh.wikipedia.org/wiki/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E5%87%A0%E4%BD%95 "欧几里得几何") 里，两条[笛卡尔坐标](https://zh.wikipedia.org/wiki/%E7%AC%9B%E5%8D%A1%E5%B0%94%E5%9D%90%E6%A0%87%E7%B3%BB "笛卡尔坐标系") 向量的点积常称为**内积**（德语：inneres Produkt；英语：inner product）。点积是**内积**的一种特殊形式：内积是点积的抽象，内积是一种双线性函数，点积是欧几里得空间（[内积空间](https://zh.wikipedia.org/wiki/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4 "内积空间")）的度量。

从代数角度看，先求两数字序列中每组对应元素的[积](https://zh.wikipedia.org/wiki/%E7%A7%AF "积")，再求所有积之和，结果即为点积。从几何角度看，点积则是两向量的[长度](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F#%E5%A4%A7%E5%B0%8F "向量") 与它们夹角[余弦](https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6 "余弦") 的积。这两种定义在笛卡尔坐标系中等价。

**点积**的名称源自表示点乘运算的[点号](https://zh.wikipedia.org/wiki/%E2%8B%85 "⋅")（a⋅b ![{\displaystyle \mathbf {a} \cdot \mathbf {b} }](assets/images/54a13e39e47048032d9f2e02dbba94f8_MD5.svg)），读作 `a dot b`，**标量积**的叫法则是在强调其运算结果为[标量](<https://zh.wikipedia.org/wiki/%E6%A0%87%E9%87%8F_(%E6%95%B0%E5%AD%A6)> "标量 (数学)") 而非[向量](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F "向量")。向量的另一种乘法是**[叉乘](https://zh.wikipedia.org/wiki/%E5%8F%89%E7%A7%AF "叉积")**（a×b ![{\displaystyle \mathbf {a} \times \mathbf {b} }](assets/images/d81d543f424edb4d90a768b2b057c495_MD5.svg)），读作 `a cross b`，其结果为向量，称为**[叉积](https://zh.wikipedia.org/wiki/%E5%8F%89%E7%A7%AF "叉积")**或**向量积**。

![](assets/images/97ad73fb9c122ee2462d3fca3548cb04_MD5.png)

## 1.10 什么是神经网络？

神经元连接成网络。神经网络是将神经单元部署成网络状而形成的。

> [!PDF|yellow] [深度学习的数学.pdf, p.18](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=31&selection=160,0,162,1&color=yellow)
>
> > 将这样的神经单元连接为网络状，就形成了神经网络。

![](assets/images/db58a2669679152f87a740e893d74351_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.19](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=32&selection=8,0,13,1&color=yellow)
>
> > 网络的连接方法多种多样，本书将主要考察作为基础的阶层型神经网络以及由其发展而来的卷积神经网络。

## 1.11 神经网络各层的职责

主要针对阶层型神经网络。输入层、隐藏层、输出层。其中

![](assets/images/79d0e9df7528b94fc6db45ee8392b998_MD5.png)

其中，（1）（2）对应：

![](assets/images/81976edca9cf8d9c1c101535f1658eee_MD5.png)

## 1.12 什么是深度学习？

> [!PDF|yellow] [深度学习的数学.pdf, p.20](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=33&selection=8,0,10,6&color=yellow)
>
> > 深度学习，顾名思义，是叠加了很多层的神经网络。叠加层有各种各样的方法，其中著名的是卷积神经网络

## 1.13 全连接层（fully connected layer）

> [!PDF|yellow] [深度学习的数学.pdf, p.21](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=34&selection=15,14,22,0&color=yellow)
>
> > 前一层的神经单元与下一层的所有神经单元都有箭头连接，这样的层构造称为全连接层（fully connected layer）

## 1.14 隐藏层的作用？


- 为何能够提取输入图像的特征呢？
- 隐藏层设置为几层比较好？

它很重要，这是多层神经网络中最维的部分。支撑整个神经网络工作的就是这个隐藏层。

> [!PDF|yellow] [深度学习的数学.pdf, p.22](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=35&selection=31,0,31,18&color=yellow)
>
> > 隐藏层具有提取输入图像的特征的作用。

> [!PDF|yellow] [深度学习的数学.pdf, p.23](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=36&selection=12,4,16,6&color=yellow)
>
> > 隐藏层肩负着特征提取（feature extraction）的重要职责

