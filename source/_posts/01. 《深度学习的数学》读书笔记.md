---
tags:
  - note
  - blog
  - 深度学习的数学
  - LLM
  - NLP
  - TODO
share: "true"
categories:
  - LLM
title: 深度学习的数学
date: 2024-11-11,17:29
comments: true
number headings: auto, first-level 1, max 6, 1.1
---

| 时间       | 备忘录                                                                                                               |
| ---------- | -------------------------------------------------------------------------------------------------------------------- |
| 2024-11-11 | 开始阅读《深度学习的数学》                                                                                           |
| 2024-11-19 | 第 1 章都是一些概念，每个概念都见过，组合在一起就看不太明白了。                                                      |
| 2024-11-20 | 第 2 章的数学基础部分还好，很多知识点大学时都学过，也都知道。<br>今天把第 4 章看完了，信息量很大，大部分是浅尝辄止。 |
|            |                                                                                                                      |
|            |                                                                                                                      |

# 1 第 1 章：神经网络的思想

## 1.1 深度学习里的“深度”是什么意思呢？

## 1.2 神经网络的基本原理？

从数学上来说，其原理十分容易。本书的目的就是阐明它的原理。可能后面会解答清楚。

## 1.3 单位阶跃函数

[单位阶跃函数](https://zh.wikipedia.org/zh-hans/%E5%8D%95%E4%BD%8D%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0)，又称赫维赛德阶跃函数，通常用 H 或 θ 表记，有时也会用 u、 1 或 𝟙 表记，**是一个由奥利弗·亥维赛提出的阶跃函数，参数为负时值为 0，参数为正时值为 1**。分段函数形式的定义如下： 另一种定义为： 它是个不连续函数，其微分是狄拉克 δ 函数。

> ([深度学习的数学.pdf, p.10](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=23&selection=275,0,275,6&color=yellow))
> 单位阶跃函数

![](assets/images/7daf74d7beeb14a0f22a8285a010602e_MD5.png)

单位阶跃函数的图形如下所示：

![](assets/images/07bf99eea13563a206b85fbe0be471bc_MD5.png)

## 1.4 Sigmoid 函数

> [!PDF|yellow] [深度学习的数学.pdf, p.11](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=24&selection=310,11,310,21&color=yellow)
>
> > Sigmoid 函数

S 型生长曲线。

[Sigmoid 函数](https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407) 也叫 [Logistic 函数](https://baike.baidu.com/item/Logistic%E5%87%BD%E6%95%B0/3520384?fromModule=lemma_inlink)，用于隐层神经元输出，取值范围为 (0,1)，它可以将一个实数映射到 (0,1) 的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid 函数为神经网络中的激励函数，是一种光滑且严格单调的饱和函数，其表达式为：

![](assets/images/5f3a4903c78383b881b3a0c10c47385f_MD5.png)

![](assets/images/51616d0e23a62d8964ec0d4aae01278e_MD5.png)

## 1.5 激活函数

在计算机网络中，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。

sigmoid 函数和 tanh 函数是研究早期被广泛使用的 2 种[激活函数](https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink)。两者都为 S 型饱和函数。当 sigmoid 函数输入的值趋于正无穷或负无穷时，梯度会趋近零，从而发生梯度弥散现象。Sigmoid 函数的输出恒为正值，不是以零为中心的，这会导致权值更新时只能朝一个方向更新，从而影响收敛速度。Tanh 激活函数是 sigmoid 函数的改进版，是以零为中心的对称函数，收敛速度快，不容易出现 loss 值晃动，但是无法解决梯度弥散的问题。2 个函数的计算量都是指数级的，计算相对复杂。Softsign 函数是 tanh 函数的改进版，为 S 型饱和函数，以零为中心，值域为（−1，1）。

> [!PDF|yellow] [深度学习的数学.pdf, p.12](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=25&selection=6,0,7,10&color=yellow)
>
> > 激活函数：将神经元的工作一般化

![](assets/images/8a05ce9af6ed3aee6e2834912fbff3c5_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.14](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=27&selection=140,0,150,0&color=yellow)
>
> > 激活函数的代表性例子是 Sigmoid 函数 σ(z)

## 1.6 权重

激活函数中对应的输入信号的系数。

![](assets/images/de7817833bed3544aacb62c8572b5062_MD5.png)

对应到神经网络中就量，每层神经元传递信号时的系数。

![](assets/images/d373b2758035b1a02a1f265bc93f6ad3_MD5.png)

## 1.7 权重的大小如何确定呢？

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=28,29,31,10&color=yellow)
>
> > 神经网络中比较重要的一点就是利用网络自学习算法来确定权重大小。

## 1.8 逻辑回归

[logistic 回归](https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92/2981575?fromModule=lemma_inlink)是一种广义线性回归（generalized linear model），因此与[多重线性回归](https://baike.baidu.com/item/%E5%A4%9A%E9%87%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/4029155?fromModule=lemma_inlink)分析有很多相同之处。

## 1.9 自变量、因变量

广义解释任何一个系统（或模型）都是由各种变量构成的，**当分析这些系统（或模型）时，可以选择研究其中一些变量对另一些变量的影响，那么选择的这些变量就称为自变量，而被影响的量就被称为因变量**。例如：分析人体这个系统中，呼吸对于维持生命的影响，那么呼吸就是自变量，而生命维持的状态被认为是因变量。

## 1.10 偏置

我们将 - θ 替换为 b。

> [!PDF|yellow] [深度学习的数学.pdf, p.16](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=29&selection=62,0,72,0&color=yellow)
>
> > 经过这样处理，式子变漂亮了，也不容易发生计算错误。这个 b 称为偏置（bias）

![](assets/images/9026eb1d57b432a91870d905579d84f7_MD5.png)

> 卧槽，为了变漂亮搞这么一手？

![](assets/images/f2b0014038c32f680d824585470cdaca_MD5.png)

## 1.11 内积

在[数学](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6 "数学") 中，**点积**（德语：Punktprodukt；英语：dot product）又称**数量积**或**标量积**（德语：Skalarprodukt；英语：scalar product），是一种接受两串等长的数字序列（通常是[坐标](https://zh.wikipedia.org/wiki/%E5%9D%90%E6%A0%87 "坐标")[向量](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F "向量")）、返回单一数字的[代数](https://zh.wikipedia.org/wiki/%E4%BB%A3%E6%95%B0 "代数")[运算](https://zh.wikipedia.org/wiki/%E8%BF%90%E7%AE%97 "运算")。

在[欧几里得几何](https://zh.wikipedia.org/wiki/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E5%87%A0%E4%BD%95 "欧几里得几何") 里，两条[笛卡尔坐标](https://zh.wikipedia.org/wiki/%E7%AC%9B%E5%8D%A1%E5%B0%94%E5%9D%90%E6%A0%87%E7%B3%BB "笛卡尔坐标系") 向量的点积常称为**内积**（德语：inneres Produkt；英语：inner product）。点积是**内积**的一种特殊形式：内积是点积的抽象，内积是一种双线性函数，点积是欧几里得空间（[内积空间](https://zh.wikipedia.org/wiki/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4 "内积空间")）的度量。

从代数角度看，先求两数字序列中每组对应元素的[积](https://zh.wikipedia.org/wiki/%E7%A7%AF "积")，再求所有积之和，结果即为点积。从几何角度看，点积则是两向量的[长度](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F#%E5%A4%A7%E5%B0%8F "向量") 与它们夹角[余弦](https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6 "余弦") 的积。这两种定义在笛卡尔坐标系中等价。

**点积**的名称源自表示点乘运算的[点号](https://zh.wikipedia.org/wiki/%E2%8B%85 "⋅")（a⋅b ![{\displaystyle \mathbf {a} \cdot \mathbf {b} }](assets/images/54a13e39e47048032d9f2e02dbba94f8_MD5.svg)），读作 `a dot b`，**标量积**的叫法则是在强调其运算结果为[标量](<https://zh.wikipedia.org/wiki/%E6%A0%87%E9%87%8F_(%E6%95%B0%E5%AD%A6)> "标量 (数学)") 而非[向量](https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F "向量")。向量的另一种乘法是**[叉乘](https://zh.wikipedia.org/wiki/%E5%8F%89%E7%A7%AF "叉积")**（a×b ![{\displaystyle \mathbf {a} \times \mathbf {b} }](assets/images/d81d543f424edb4d90a768b2b057c495_MD5.svg)），读作 `a cross b`，其结果为向量，称为**[叉积](https://zh.wikipedia.org/wiki/%E5%8F%89%E7%A7%AF "叉积")**或**向量积**。

![](assets/images/97ad73fb9c122ee2462d3fca3548cb04_MD5.png)

## 1.12 什么是神经网络？

神经元连接成网络。神经网络是将神经单元部署成网络状而形成的。

> [!PDF|yellow] [深度学习的数学.pdf, p.18](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=31&selection=160,0,162,1&color=yellow)
>
> > 将这样的神经单元连接为网络状，就形成了神经网络。

![](assets/images/db58a2669679152f87a740e893d74351_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.19](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=32&selection=8,0,13,1&color=yellow)
>
> > 网络的连接方法多种多样，本书将主要考察作为基础的阶层型神经网络以及由其发展而来的卷积神经网络。

## 1.13 神经网络各层的职责

主要针对阶层型神经网络。输入层、隐藏层、输出层。其中

![](assets/images/79d0e9df7528b94fc6db45ee8392b998_MD5.png)

其中，（1）（2）对应：

![](assets/images/81976edca9cf8d9c1c101535f1658eee_MD5.png)

## 1.14 什么是深度学习？

> [!PDF|yellow] [深度学习的数学.pdf, p.20](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=33&selection=8,0,10,6&color=yellow)
>
> > 深度学习，顾名思义，是叠加了很多层的神经网络。叠加层有各种各样的方法，其中著名的是卷积神经网络

## 1.15 全连接层（fully connected layer）

> [!PDF|yellow] [深度学习的数学.pdf, p.21](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=34&selection=15,14,22,0&color=yellow)
>
> > 前一层的神经单元与下一层的所有神经单元都有箭头连接，这样的层构造称为全连接层（fully connected layer）

## 1.16 隐藏层的作用？

- 为何能够提取输入图像的特征呢？
- 隐藏层设置为几层比较好？

它很重要，这是多层神经网络中最维的部分。支撑整个神经网络工作的就是这个隐藏层。

> [!PDF|yellow] [深度学习的数学.pdf, p.22](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=35&selection=31,0,31,18&color=yellow)
>
> > 隐藏层具有提取输入图像的特征的作用。

> [!PDF|yellow] [深度学习的数学.pdf, p.23](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=36&selection=12,4,16,6&color=yellow)
>
> > 隐藏层肩负着特征提取（feature extraction）的重要职责

## 1.17 消除噪声处理？

就是前面提到的偏置。

- [ ] 大家说的调参，是它么？

> [!PDF|yellow] [深度学习的数学.pdf, p.29](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=42&selection=61,20,64,0&color=yellow)
>
> > 具体来说，将偏置放在恶魔的心中，以忽略少量的噪声。这个“心的偏置”是各个恶魔固有的值 （也就是个性）

## 1.18 恶魔之间的“交情”表示权重。心的偏置？恶魔的工作？

<font color="#ff0000">这就是 “由神经网络中的关系得出答案”的思想。</font>

![](assets/images/fb4a3e9624661040874e8f4f07a31d24_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.33](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=46&selection=73,7,75,0&color=yellow)
>
> > 因此需要禁止这样的信号并使信号变清晰，这样的功能就是偏置，在恶魔组织中表现为“心的偏置”

## 1.19 模型的合理性

求解出多层神经网络实现工作权重和偏置。

> [!PDF|yellow] [深度学习的数学.pdf, p.34](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=47&selection=8,21,10,10&color=yellow)
>
> > 并能够充分地解释所给出的数据，就能够验证以上话题的合理性。这需要数学计算，必须将语言描述转换为数学式。

## 1.20 隐藏层需要多少个神经元？

书中说，存在某种预估？

> 这个就有有点玄学了啊！规模大的时候如何估呢！？

- [ ] 关于具体的确认方法，将在第 3 章解答。

## 1.21 人工智能研究中的几次热潮

![](assets/images/7692ecc71d03e2e80b96e8ba782a179c_MD5.png)

## 1.22 神经网络参数确定的方法

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=35,0,35,25&color=yellow)
>
> > 神经网络的参数确定方法分为有监督学习和无监督学习。

## 1.23 学习数据

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=36,7,38,4&color=yellow)
>
> > 有监督学习是指，为了确定神经网络的权重和偏置，事先给予数据，这些数据称为学习数据

## 1.24 学习

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=39,1,44,14&color=yellow)
>
> > 根据给定的学习数据确定权重和偏置，称为学习。注：学习数据也称为训练数据。

## 1.25 最优化

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=46,3,50,0&color=yellow)
>
> > 神经网络是怎样学习的呢？思路极其简单：计算神经网络得出的预测值与正解的误差，确定使得误差总和达到最小的权重和偏置。这在数学上称为模型的最优化

## 1.26 代价函数（cost function）

用符号 CT 表示（T 是 Total 的首字母）。

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=52,0,61,0&color=yellow)
>
> > 关于预测值与正解的误差总和，有各种各样的定义。本书采用的是最古典的定义：针对全部学习数据，计算预测值与正解的误差的平方（称为平方误差），然后再相加。这个误差的总和称为代价函数（cost function）

## 1.27 最小二乘法

- [ ] 我们将在 2 - 12 节以回归分析为例来具体考察什么是最小二乘法。

> [!PDF|yellow] [深度学习的数学.pdf, p.36](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=49&selection=75,0,80,6&color=yellow)
>
> > 利用平方误差确定参数的方法在数学上称为最小二乘法，它在统计学中是回归分析的常规手段。

# 2 第 2 章：神经网络的数学基础

## 2.1 一次函数

![](assets/images/dda8899150681105af5cd0e98ed9ba76_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.41](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=54&selection=71,17,72,8&color=yellow)
>
> > 神经单元的加权输入可以表示为一次函数关系

![](assets/images/056b94377c408764169b8589908667df_MD5.png)

## 2.2 二次函数

![](assets/images/eccaca8f7eed68052e09a56708d100d3_MD5.png)

## 2.3 单位阶跃函数

![](assets/images/dca5744f1efb73b4c03403e0b4935cbf_MD5.png)

## 2.4 指数函数与 Sigmoid 函数

![](assets/images/ffa498a270e27e02f0c9a6de71a760d8_MD5.png)

## 2.5 正态分布的概率密度函数

![](assets/images/6fd9d451456d6b1d88b81723c6ec247c_MD5.png)

## 2.6 数列

> [!PDF|yellow] [深度学习的数学.pdf, p.46](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=59&selection=70,0,75,1&color=yellow)
>
> > 在神经网络的世界中出现的数列是有限项的数列。这样的数列称为有穷数列。在有穷数列中，数列的最后一项称为末项。

## 2.7 数列的通项公式

> [!PDF|yellow] [深度学习的数学.pdf, p.47](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=60&selection=16,0,27,1&color=yellow)
>
> > 将数列的第 n 项用一个关于 n 的式子表示出来，这个式子就称为该数列的通项公式。

![](assets/images/c542f19dcd124393cdded20d0394db1d_MD5.png)

![](assets/images/a01ef2af1c8234344ef8faa32b73b752_MD5.png)

## 2.8 递推关系式

即，数列的递推公式。

## 2.9 向量

> [!PDF|yellow] [深度学习的数学.pdf, p.54](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=67&selection=8,0,12,1&color=yellow)
>
> > 把向量的箭头放在坐标平面上，就可以用坐标的形式表示向量。把箭头的起点放在原点，用箭头终点的坐标表示向量，这叫作向量的坐标表示。

> [!PDF|yellow] [深度学习的数学.pdf, p.55](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=68&selection=10,0,13,1&color=yellow)
>
> > 从直观上来讲，表示向量的箭头的长度称为这个向量的大小。

向量的内积：
![](assets/images/d9224ba747ed217621e2b16238b3c832_MD5.png)

## 2.10 柯西-施瓦茨不等式

![](assets/images/8feed7029853223fda8b4c27208b1056_MD5.png)

![](assets/images/bd8e4ea0b19ecb904f5ee6f3dae10029_MD5.png)

- [ ] 性质 ① 就是后述的梯度下降法的基本原理。
- [ ] 后面我们考察卷积神经网络时，这个观点就变得十分重要（附录 C）。

## 2.11 矩阵

> [!PDF|yellow] [深度学习的数学.pdf, p.63](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=76&selection=338,11,338,22&color=yellow)
>
> > 矩阵的乘法不满足交换律

> [!PDF|yellow] [深度学习的数学.pdf, p.63](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=76&selection=347,0,355,11&color=yellow)
>
> > 而单位矩阵 E 与任意矩阵 A 的乘积都满足以下交换律

转置矩阵。

## 2.12 Hadamard 乘积

在[数学](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6 "数学")中，**[阿达马矩阵](https://zh.wikipedia.org/zh-cn/%E9%98%BF%E8%BE%BE%E9%A9%AC%E7%9F%A9%E9%98%B5)**（英语：**Hadamard matrix**）是一种特殊的正交矩阵，具有一些重要的性质和应用。它的特点是每个元素只能是+1 或-1，并且每行（或每列）之间的内积为 0，表示彼此正交。Hadamard 矩阵的大小为 2 的幂次方，即维度为 ![{\displaystyle 2^{n}\times 2^{n}}](assets/images/76aa8646165fe13881431f213a24a4d8_MD5.svg)。

阿达马矩阵常用于[纠错码](https://zh.wikipedia.org/wiki/%E7%BA%A0%E9%94%99%E7%A0%81 "纠错码")，如  [Reed-Muller 码](https://zh.wikipedia.org/w/index.php?title=Reed-Muller%E7%A0%81&action=edit&redlink=1)。阿达马矩阵的命名来自于法国数学家[雅克·阿达马](https://zh.wikipedia.org/wiki/%E9%9B%85%E5%85%8B%C2%B7%E9%98%BF%E8%BE%BE%E9%A9%AC "雅克·阿达马")。

## 2.13 导数

> [!PDF|yellow] [深度学习的数学.pdf, p.69](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=82&selection=170,0,195,0&color=yellow)
>
> > 当函数 f (x) 在 x = a 处取得最小值时，f' (a) = 0。

> [!PDF|yellow] [深度学习的数学.pdf, p.69](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=82&selection=258,0,283,12&color=yellow)
>
> > f' (a) = 0 是函数 f (x) 在 x = a 处取得最小值的必要条件。

![](assets/images/941a6fa34e0ab406fd1754637ec723e3_MD5.png)

## 2.14 偏导数基础

> [!PDF|yellow] [深度学习的数学.pdf, p.72](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=85&selection=44,30,48,1&color=yellow)
>
> > 本节我们来考察有两个以上的自变量的函数。有两个以上的自变量的函数称为多变量函数。

> [!PDF|yellow] [深度学习的数学.pdf, p.73](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=86&selection=82,25,88,0&color=yellow)
>
> > 关于某个特定变量的导数就称为偏导数（partial derivative）

![](assets/images/97483ca20402c510cab1db48e362857d_MD5.png)

## 2.15 多变量函数的最小值条件

> [!PDF|yellow] [深度学习的数学.pdf, p.74](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=87&selection=89,0,109,0&color=yellow)
>
> > 光滑的单变量函数 y = f (x) 在点 x 处取得最小值的必要条件是导函数在该点取值 0

![](assets/images/c6f1d4777b8e89135c4da67982e406a2_MD5.png)

## 2.16 拉格朗日乘数法

λ：拉姆达

![](assets/images/e52bd9e2cca003ba76676d3e376d9f57_MD5.png)

## 2.17 复合函数

![](assets/images/eb1d027b4ac66604669bf1738b4ecf79_MD5.png)

## 2.18 复合函数求导公式

![](assets/images/b09221e5c5fd86b6b77b4a194c967d15_MD5.png)

## 2.19 多变量函数的近似公式

![](assets/images/e6175cc0f521cc1aee13c6fd012ab9bf_MD5.png)
![](assets/images/50cdf09c329f4dfbf78e598f9949bae6_MD5.png)

## 2.20 梯度下降法的含义与公式

> [!PDF|yellow] [深度学习的数学.pdf, p.84](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=97&selection=77,0,79,1&color=yellow)
>
> > 在数值分析领域，梯度下降法也称为最速下降法。

![](assets/images/44d4b7e33f5f865fad945aecda61919a_MD5.png)

![](assets/images/4d78e1e1dbcd2b96beb7518b4f073137_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.88](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=101&selection=26,19,40,1&color=yellow)
>
> > 这种寻找函数 f (x, y) 的最小值点的方法称为二变量函数的梯度下降法。

## 2.21 哈密顿算子 ∇

> [!PDF|yellow] [深度学习的数学.pdf, p.89](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=102&selection=157,0,162,0&color=yellow)
>
> > ∇ 称为哈密顿算子

![](assets/images/b2606cbd59db5d2b67e060f3f9ccf162_MD5.png)

## 2.22 梯度下降法的要点

> [!PDF|yellow] [深度学习的数学.pdf, p.90](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=103&selection=135,0,142,7&color=yellow)
>
> > η 只是简单地表示正的微小常数。而在实际使用计算机进行计算时，如何恰当地确定这个 η 是一个大问题。

> [!PDF|yellow] [深度学习的数学.pdf, p.90](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=103&selection=172,0,178,22&color=yellow)
>
> > 在神经网络的世界中，η 称为学习率。遗憾的是，它的确定方法没有明确的标准，只能通过反复试验来寻找恰当的值。

## 2.23 最优化问题

> [!PDF|yellow] [深度学习的数学.pdf, p.94](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=107&selection=9,0,18,1&color=yellow)
>
> > 在为了分析数据而建立数学模型时，通常模型是由参数确定的。在数学世界中，最优化问题就是如何确定这些参数。
> >
> > 从数学上来说，确定神经网络的参数是一个最优化问题，具体就是对神经网络的参数（即权重和偏置）进行拟合，使得神经网络的输出与实际数据相吻合。
> >
> > 为了理解最优化问题，最浅显的例子就是回归分析。

## 2.24 回归分析

> [!PDF|yellow] [深度学习的数学.pdf, p.94](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=107&selection=23,0,26,1&color=yellow)
>
> > 由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量，这样的方法称为回归分析。

![](assets/images/c4eb0bca3d4f32e6628a77667144ece6_MD5.png)

![](assets/images/2ff26a3f53e89f24254400b5ae16a203_MD5.png)

![](assets/images/cb892479eee0b177867b85649eb89ffd_MD5.png)

回归分析认为，p、q 是使误差总和式 (6) 最小的解。接下来求偏导就行了。

## 2.25 代价函数（损失函数）

> [!PDF|yellow] [深度学习的数学.pdf, p.98](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=111&selection=106,0,118,6&color=yellow)
>
> > 在最优化方面，误差总和 CT 可以称为“误差函数”“损失函数”“代价函数”等。本书采用代价函数（cost function）这个名称。

> [!PDF|yellow] [深度学习的数学.pdf, p.98](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=111&selection=131,14,144,7&color=yellow)
>
> > 利用平方误差的总和 CT 进行最优化的方法称为最小二乘法。本书中我们只考虑将平方误差的总和 CT 作为代价函数。

## 2.26 模型参数的个数

> [!PDF|yellow] [深度学习的数学.pdf, p.99](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=112&selection=98,6,121,15&color=yellow)
>
> > 回归方程是根据大量的条件所得到的折中结果。这里所说的“折中”是指，理想中应该取值 0 的代价函数式 (6) 只能取最小值。因此，模型与数据的误差 CT 不为 0 也无须担心。不过，只要误差接近 0，就可以说这是合乎数据的模型。

# 3 神经网络的最优化

## 3.1 参数和变量

> [!PDF|yellow] [深度学习的数学.pdf, p.102](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=115&selection=28,1,31,1&color=yellow)
>
> > 像权重和偏置这种确定数学模型的常数称为模型的参数。

![](assets/images/23717427dd9c4c6f52746042e9e35398_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.103](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=116&selection=117,0,139,1&color=yellow)
>
> > 首先，我们对层进行编号，如下图所示，最左边的输入层为层 1，隐藏层（中间层）为层 2、层 3……最右边的输出层为层 l（这里的 l 指 last 的首字母，表示层的总数）。

注意：权重 w 的下标表示。

![](assets/images/31c1abc40e968c9f864d02640456aebc_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.107](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=120&selection=158,0,158,8&color=yellow)
>
> > 变量值的表示方法

![](assets/images/41d9d5a86281b4740086afd0d8906058_MD5.png)

## 3.2 神经网络的变量的关系式

![](assets/images/1f5509ef836a32990cce0729dfac8615_MD5.png)

![](assets/images/276ba43b74e4b2f5e588d16358788faa_MD5.png)

![](assets/images/93e2fb82ef689d321eb439571f7319ef_MD5.png)

![](assets/images/0982d89f6325b072bfae1a719225a494_MD5.png)

![](assets/images/ef349a07937a6f72cff8e0e7e7fd53d0_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.113](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=126&selection=217,0,217,12&color=yellow)
>
> > 神经网络的变量的矩阵表示

![](assets/images/1ac8e5655821d53cd21784f1995537a2_MD5.png)

## 3.3 神经网络的学习数据和正解

> [!PDF|yellow] [深度学习的数学.pdf, p.114](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=127&selection=14,0,28,22&color=yellow)
>
> > 利用事先提供的数据（学习数据）来确定权重和偏置，这在神经网络中称为学习（1 - 7 节）。学习的逻辑非常简单，使得神经网络算出的预测值与学习数据的正解的总体误差达到最小即可。

> [!PDF|yellow] [深度学习的数学.pdf, p.115](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=128&selection=104,20,105,17&color=yellow)
>
> > 经网络的情况下，则通常无法将预测值和正解整合在一张表里。

> [!PDF|yellow] [深度学习的数学.pdf, p.116](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=129&selection=20,3,20,18&color=yellow)
>
> > 如何将这些正解教给神经网络呢？

> [!PDF|yellow] [深度学习的数学.pdf, p.116](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=129&selection=25,0,25,25&color=yellow)
>
> > 神经网络的预测值用输出层神经单元的输出变量来表示。

加正解变量：

> [!PDF|yellow] [深度学习的数学.pdf, p.117](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=130&selection=40,1,48,11&color=yellow)
>
> > 那么如何将 1 个正解和 2 个输出变量对应起来呢？

![](assets/images/0a604288d3992aba9e6e9189594db8fb_MD5.png)

## 3.4 交叉熵

![](assets/images/f32a6cf8c05d68189f55de01cc238ce6_MD5.png)

## 3.5 最优化

> [!PDF|yellow] [深度学习的数学.pdf, p.119](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=132&selection=17,0,41,1&color=yellow)
>
> > 用于数据分析的数学模型是由参数确定的。在神经网络中，权重和偏置就是这样的参数。通过调整这些参数，使模型的输出符合实际的数据（在神经网络中就是学习数据），从而确定数学模型，这个过程在数学上称为最优化（2 - 12 节），在神经网络的世界中则称为学习（1 - 7 节）。

> [!PDF|yellow] [深度学习的数学.pdf, p.119](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=132&selection=42,3,44,10&color=yellow)
>
> > 参数是怎样确定的呢？其原理非常简单，具体方法就是，对于全部数据，使得从数学模型得出的理论值（本书中称为预测值）与实际值的误差达到最小。

![](assets/images/5c89c60e99231da03b67f5e6c27e9213_MD5.png)

# 4 第 4 章：神经网络和误差所向传播法

## 4.1 函数最小值

> [!PDF|yellow] [深度学习的数学.pdf, p.134](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=147&selection=28,0,28,27&color=yellow)
>
> > 求函数最小值的通用方法中，最有名的就是利用最小值条件。

![](assets/images/a5c2a16cd908d59ae047fbc6433ee161_MD5.png)

但是，在规模比较大的时候方程会很多，求解方程极其困难。于是，<font color="#ff0000">梯度下降法应运而生。</font>

## 4.2 在神经网络中应用梯度下降法

![](assets/images/cde35a35aa46600cc99148faec28c414_MD5.png)

![](assets/images/0c2e4552df47d6b0593daeb0b086f7fb_MD5.png)

梯度下降虽然香，但是对平方误差求导却很麻烦。于是，<font color="#ff0000">有了误差所向传播法</font>。

> [!PDF|yellow] [深度学习的数学.pdf, p.139](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=152&selection=453,7,456,6&color=yellow)
>
> > 用具体的式子来求梯度分量是一件非常困难的工作。虽然单个的计算比较简单，但是会被导数的复杂与繁多所压倒，进入所谓“导数地狱”的世界。为了解决这个问题，人们研究出了误差反向传播法。

先求导再求和：

> [!PDF|yellow] [深度学习的数学.pdf, p.139](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=152&selection=476,0,476,19&color=yellow)
>
> > 梯度分量是一个一个学习实例的简单的和。

![](assets/images/802147798b277592d0aab8f619af54b4_MD5.png)

## 4.3 误差反向传播法（BP 法）

![](assets/images/dea493bd9e7a7f15147334fd4a08e286_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.141](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=154&selection=18,0,20,17&color=yellow)
>
> > 梯度下降法对于寻找多变量函数的最小值的问题是有效的。然而在神经网络的世界中，变量、参数和函数错综复杂，无法直接使用梯度下降法，于是就出现了误差反向传播法。

![](assets/images/7da342e5bc0ec95193bc937fa0a3a96c_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.146](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=159&selection=144,0,145,2&color=yellow)
>
> > 误差反向传播法的特点是将繁杂的导数计算替换为数列的递推关系式。

## 4.4 神经单元误差

![](assets/images/2fec34449ff36c40b766e85b77b762f6_MD5.png)

![](assets/images/f45c49f8c554f2d50abddfa713e56f51_MD5.png)

> [!PDF|yellow] [深度学习的数学.pdf, p.148](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=161&selection=134,4,139,15&color=yellow)
>
> > 如果给出平方误差 C 和激活函数，就可以具体地求出相当于“末项”的输出层神经单元误差

![](assets/images/b04aa04f4096bf82f5aa4120cc565dbd_MD5.png)

![](assets/images/45d40764ee4b23a2bc9a57a84660237d_MD5.png)

![](assets/images/82730074f5c3eed96bc0c2395817f491_MD5.png)

## 4.5 反向传播算法计算神经网络的权重和偏置

![](assets/images/9410ba9309c1c45e40ec7787004f8527_MD5.png)

![](assets/images/89dd21a7f8bb57278ba3844ad1637857_MD5.png)

- 4-1 节式（2）：

![](assets/images/77a44c594f7337bcb8ecdb120226792b_MD5.png)

- 4-1 节式（3）：

![](assets/images/925cc77ca4beba1e058a7f59b203e8f3_MD5.png)

- 层的神经网络误差计算：

![](assets/images/0981c916b9ab69f6f68e7963ba155bb3_MD5.png)

- 4-3 节式（16）：

![](assets/images/85f83b89b4b573a67f3ef5bf916bc898_MD5.png)

- 4-1 节式（9）：

![](assets/images/d46b383ed312d939e7ea947584347ddc_MD5.png)

- 4-2 节式（11）：

![](assets/images/7a73aa0ed0f41c713e32c730544c21e7_MD5.png)

# 5 第 5 章：深度学习和卷积神经网络

## 5.1 神经网络的结构

> [!PDF|yellow] [深度学习的数学.pdf, p.168](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=181&selection=43,0,51,9&color=yellow)
> > 图中用圆圈将变量名圈起来的就是神经单元，从这个图中我们可以了解到卷积神经网络的特点。隐藏层由多个具有结构的层组成。具体来说，隐藏层是多个由卷积层和池化层构成的层组成的。它不仅“深”，而且含有内置的结构。
> 
> 

## 5.2 卷积层

> [!PDF|yellow] [深度学习的数学.pdf, p.168](2%20Aera/IT/LLM/99.%20资料/书籍/深度学习的数学.pdf.pdf#page=181&selection=53,2,53,28&color=yellow)
> > 卷积层的英文是 convolution layer。


- [ ] 什么是卷积？


## 5.3 神经网络的设计思想



## 5.4 TODO
## 5.5 TODO
## 5.6 TODO
## 5.7 TODO
## 5.8 TODO
## 5.9 TODO



# 6 附录

## 6.1 TODO
